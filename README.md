# Tessera

Tessera is a next-generation programming model and compiler stack for deep learning, HPC, and scientific workloads.  
It introduces a multi-level IR (Graph â†’ Schedule â†’ Tile â†’ Target) and a DSL for operator-based modeling, enabling deterministic, scalable, and portable execution across NVIDIA, AMD, Intel, and CPU backends.

---

## ğŸ” Overview Diagrams

### 1. Tessera Execution Flow

![Tessera Overview](docs/overview/tessera_overview.png)

- **Graph IR**: Operator algebra, autodiff, symbolic transformations  
- **Schedule IR**: Fusion, tiling, autotuning, pipeline scheduling  
- **Tile IR**: Explicit GPU mapping (blocks, warps, Tensor Cores)  
- **Target IR**: Lowered to PTX, ROCm LLVM, Intel Level Zero, CPU LLVM  

---

### 2. Tessera Execution Hierarchy (like CUDAâ€™s Grid/Block/Thread Pyramid)

![Tessera Hierarchy](docs/overview/tessera_hierarchy.png)

- **Graph Level** â†’ Operator graph, autodiff  
- **Schedule Level** â†’ Fusion, tiling, pipeline  
- **Tile Level** â†’ Block/thread mapping, Tensor Core tiles  
- **Thread Level** â†’ Individual GPU threads, SIMT execution  

---

## ğŸ“š Documentation

The full documentation set is organized by topic:

- **[Programming Guide](docs/Programming_Guide/)** â€“ Core language features and usage  
- **[Performance Best Practices](docs/Performance/)** â€“ Occupancy, memory tuning, autotuning  
- **[Numerical Behavior Guide](docs/Numerical_Behavior/)** â€“ Determinism, stability, mixed precision  
- **[Interop & Tooling Guide](docs/Tools_Interop/)** â€“ Python, C++, MLIR, debuggers, profilers  
- **[Hardware Mapping Guide](docs/Hardware_Mapping_Guide/)** â€“ Mapping Tessera onto GPUs  
- **[Tutorials Volume](docs/Tutorials/)** â€“ Hands-on walkthroughs  
- **[Operator Reference](docs/Reference/Tessera_Operator_Reference.md)** â€“ Operator catalog  
- **[Runtime & ABI Spec](docs/Runtime_ABI/)** â€“ Normative runtime and ABI specification  
- **[IR Specifications](docs/IR_Documentation/)** â€“ Graph IR, Schedule IR, Tile IR, Target IR 
- **[Uncertenty & Robustness Guide](docs/Tessera_Uncertainty_Robustness/docs/)** - Predictive uncertainty capabilites
- **[Lifelong Learning](docs/Tessera_Lifelong_Learning_Package/docs/)** - Lifelong Learning Solutions 
- **[Interpretability Artifacts](docs/Tessera_Interpretability_Package/docs/)** - prediction can return feature attributions, concept relevance, counterfactuals, and causal structure
- **[Differentiable Architecture Search](docs/Tessera_DNAS_Package/docs/)** - Differentiable Neural Architecture Search (DNAS)
- **[Quality Assurance & Testing](docs/Quality_Assurance_Testing/)** - Q&A Testing Guide
- **[PyTorch Bridge](docs/Tessera_PyTorch_Bridge/docs/)** - Tessera Pytorch Language Bridge
- **[Learning Specification Language (LSL)](docs/Tessera_LSL_Package/docs/)** - Declaritive Language which allows you describe Higher 
level Learning Abstractions 
- **[Probabilistic Programming](docs/Tessera_ProbProg_Package/docs/)** - Uncertainty and probabilistic reasoning support 
- **[Shape Debugging System](docs/Tessera_Shape_System_Package/)** -  Shape System with Compile-Time Verification 

# Tessera Model Examples

This section provides **end-to-end examples** of mapping real-world models and reasoning frameworks onto the Tessera programming model.  
Each example illustrates how Tesseraâ€™s multi-level IR (Graph IR â†’ Schedule IR â†’ Tile IR â†’ Target IR) and runtime abstractions support scalable, efficient, and interpretable model design.  

---

## Available Examples

- **[Hierarchical Reasoning Model (HRM)](HRM/Tessera_HRM_Mapping.md)**  
  Mapping HRMâ€™s Plannerâ€“Decomposerâ€“Executor architecture into Tesseraâ€™s Graph IR and Schedule IR layers. Includes diagram of HRM â†” Tessera IR mapping.

- **[GPT-OSS-120B](GPT-OSS-120B/Tessera_GPTOSS_Example.md)**  
  Illustrates how Tessera handles large-scale transformer models with distributed tensors, checkpointing, and efficient sharding strategies.

- **[Physics-Informed Neural Networks (PINN)](PINN/Tessera_PINN_NavierStokes.md)**  
  A 2D Navierâ€“Stokes example with incompressibility constraints. Demonstrates Tesseraâ€™s operator adjoints for coupled PDE systems.

- **[Spectral Mixture-of-Experts (Spectral-MoE)](MoE/Tessera_Spectral_MoE.md)**  
  Combines FFT-based spectral decomposition, recursive operators, and MoE routing. Highlights Tesseraâ€™s strength in operator factorization and distributed expert parallelism.


---

## ğŸš€ Quick Example

```python
from tessera import op, dist

# Create a distributed mesh across 8 GPUs
mesh = dist.Mesh(axes=["dp"], devices=range(8))

# Define a sharded tensor
X = dist.tensor((1024, 1024), layout=dist.ShardSpec(("row",), ("dp",)), mesh=mesh)

# Apply a fused operator pipeline
Y = op.pipeline([
    op.matmul(X, X.T),
    op.relu,
    op.layernorm
])
```

---

## ğŸ“Œ Repository Structure

```
tessera/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ overview/                 # High-level diagrams
â”‚   â”œâ”€â”€ programming_guide/        # Main programming guide
â”‚   â”œâ”€â”€ performance/              # Performance best practices
â”‚   â”œâ”€â”€ numerical/                # Numerical behavior guide
â”‚   â”œâ”€â”€ runtime_abi/              # Runtime & ABI spec
â”‚   â”œâ”€â”€ hardware_mapping/         # GPU mapping guide
â”‚   â”œâ”€â”€ tutorials/                # Hands-on tutorials
â”‚   â”œâ”€â”€ interop/                  # Interop & tooling guide
â”‚   â””â”€â”€ reference/                # Operator reference
â””â”€â”€ README.md                     # This file
```

```
docs/models_examples/
 â”œâ”€â”€ HRM/  
 â”‚    â”œâ”€â”€ Tessera_HRM_Mapping.md  
 â”‚    â””â”€â”€ Tessera_HRM_Mapping_Diagram.png  
 â”œâ”€â”€ GPT-OSS-120B/  
 â”‚    â””â”€â”€ Tessera_GPTOSS_Example.md  
 â”œâ”€â”€ PINN/  
 â”‚    â””â”€â”€ Tessera_PINN_NavierStokes.md  
 â”œâ”€â”€ MoE/  
 â”‚    â””â”€â”€ Tessera_Spectral_MoE.md  
 â””â”€â”€ README.md   â† index listing all example models
 ```

Tessera Standard Operator Library (TSOL)
```
docs/TSOL/
â”œâ”€â”€ TSOL_Guide.md
â”œâ”€â”€ tessera/
â”‚   â””â”€â”€ ops.pyi
â”œâ”€â”€ include/
â”‚   â””â”€â”€ tessera/
â”‚       â””â”€â”€ ops.hpp
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_stubs.py
â””â”€â”€ mypy.ini
```
Tessera Collectives & Distributed Systems Guide
```
docs/Tessera_Collectives_Distributed/
â”œâ”€â”€ Tessera_Collectives_Distributed.md
â”œâ”€â”€ README.md
â””â”€â”€ images/
    â”œâ”€â”€ mesh_collectives.png
    â””â”€â”€ zero_flow.png
```
---

## ğŸ”® Roadmap

- Expand operator libraries (cuBLAS, cuDNN, cuFFT equivalents in Tessera).  
- Add integration with Hugging Face models for inference.  
- Optimize distributed training at 128+ node scale.  
- Extend tooling (profiler, debugger, autotuner caches).  
